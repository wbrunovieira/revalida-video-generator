---
# Setup CogVideoX with isolated venv - T2V and I2V support
# Run with: ansible-playbook playbook.yml --tags setup-cogvideox
#
# Creates: /mnt/models/CogVideoX-venv (isolated virtualenv)
# Models:
#   - THUDM/CogVideoX-5b (~10GB) - Text-to-Video
#   - THUDM/CogVideoX1.5-5B-I2V (~15GB) - Image-to-Video
# Features: Multi-GPU via device_map, INT8 quantization option
#
# Storage: Uses /mnt/models (500GB EBS mounted via mount-ebs.sh)

- name: Display CogVideoX setup banner
  debug:
    msg: |
      ==========================================
      üé¨ SETTING UP COGVIDEOX (T2V + I2V)
      ==========================================
      Models:
      - CogVideoX-5b (T2V): 49 frames @ 8fps (6s)
      - CogVideoX1.5-5B-I2V: 81 frames @ 8fps (10s)

      Features:
      - BF16 precision (recommended)
      - Multi-GPU support via device_map
      - INT8 quantization option for low VRAM
      - Isolated venv: {{ models_dir }}/CogVideoX-venv
      ==========================================
  tags: [setup-cogvideox]

# ============================================
# STEP 1: CREATE ISOLATED COGVIDEOX-VENV
# ============================================
- name: Check if CogVideoX-venv exists
  stat:
    path: "{{ models_dir }}/CogVideoX-venv/bin/activate"
  register: cogvideox_venv_check
  tags: [setup-cogvideox]

- name: Remove old CogVideoX-venv if broken
  file:
    path: "{{ models_dir }}/CogVideoX-venv"
    state: absent
  when: not cogvideox_venv_check.stat.exists
  tags: [setup-cogvideox]

- name: Create CogVideoX-venv
  command: python3 -m venv {{ models_dir }}/CogVideoX-venv
  become_user: ubuntu
  when: not cogvideox_venv_check.stat.exists
  tags: [setup-cogvideox]

- name: Display venv status
  debug:
    msg: "‚úÖ CogVideoX-venv created: {{ models_dir }}/CogVideoX-venv"
  tags: [setup-cogvideox]

# ============================================
# STEP 2: INSTALL DEPENDENCIES
# ============================================
- name: Upgrade pip, wheel, setuptools in CogVideoX-venv
  pip:
    name:
      - pip
      - wheel
      - setuptools
    state: latest
    virtualenv: "{{ models_dir }}/CogVideoX-venv"
  become_user: ubuntu
  tags: [setup-cogvideox]

- name: Install PyTorch with CUDA 12.1
  shell: |
    source {{ models_dir }}/CogVideoX-venv/bin/activate
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
  become_user: ubuntu
  args:
    executable: /bin/bash
  tags: [setup-cogvideox]

- name: Display PyTorch install status
  debug:
    msg: "‚úÖ PyTorch installed with CUDA 12.1"
  tags: [setup-cogvideox]

# ============================================
# STEP 3: INSTALL DIFFUSERS AND DEPENDENCIES
# ============================================
- name: Install CogVideoX dependencies
  pip:
    name:
      - diffusers>=0.31.0
      - transformers>=4.46.2
      - accelerate>=1.1.1
      - imageio-ffmpeg>=0.5.1
      - imageio
      - sentencepiece
      - huggingface-hub
      - numpy
      - scipy
      - Pillow
    virtualenv: "{{ models_dir }}/CogVideoX-venv"
  become_user: ubuntu
  tags: [setup-cogvideox]

- name: Display dependencies status
  debug:
    msg: "‚úÖ Diffusers and dependencies installed"
  tags: [setup-cogvideox]

# ============================================
# STEP 4: DOWNLOAD COGVIDEOX-5B MODEL (T2V)
# ============================================
- name: Check if CogVideoX-5b model exists
  stat:
    path: "{{ models_dir }}/CogVideoX-5b/model_index.json"
  register: cogvideox_t2v_check
  tags: [setup-cogvideox]

- name: Download CogVideoX-5b model for T2V (~10GB)
  shell: |
    source {{ models_dir }}/CogVideoX-venv/bin/activate
    cd {{ models_dir }}
    echo "üì• Downloading CogVideoX-5b (~10GB, will take 10-20 minutes)..."
    huggingface-cli download THUDM/CogVideoX-5b --local-dir CogVideoX-5b
  become_user: ubuntu
  when: not cogvideox_t2v_check.stat.exists
  tags: [setup-cogvideox]
  args:
    executable: /bin/bash
  async: 3600
  poll: 30

- name: Verify T2V model downloaded successfully
  stat:
    path: "{{ models_dir }}/CogVideoX-5b/model_index.json"
  register: cogvideox_t2v_final
  tags: [setup-cogvideox]

- name: Display T2V model download status
  debug:
    msg: |
      CogVideoX-5b (T2V): {{ 'Downloaded ‚úÖ' if cogvideox_t2v_final.stat.exists else 'Failed ‚ùå' }}
      Location: {{ models_dir }}/CogVideoX-5b/
  tags: [setup-cogvideox]

# ============================================
# STEP 5: DOWNLOAD COGVIDEOX1.5-5B-I2V MODEL
# Downloads directly to /mnt/models (500GB EBS)
# ============================================
- name: Check if CogVideoX1.5-5B-I2V model exists
  stat:
    path: "{{ models_dir }}/CogVideoX1.5-5B-I2V/model_index.json"
  register: cogvideox_i2v_check
  tags: [setup-cogvideox]

- name: Download CogVideoX1.5-5B-I2V model (~15GB)
  shell: |
    source {{ models_dir }}/CogVideoX-venv/bin/activate
    cd {{ models_dir }}
    echo "üì• Downloading CogVideoX1.5-5B-I2V (~15GB, will take 15-30 minutes)..."
    huggingface-cli download THUDM/CogVideoX1.5-5B-I2V --local-dir CogVideoX1.5-5B-I2V
  become_user: ubuntu
  when: not cogvideox_i2v_check.stat.exists
  tags: [setup-cogvideox]
  args:
    executable: /bin/bash
  async: 3600
  poll: 30

- name: Verify I2V model downloaded successfully
  stat:
    path: "{{ models_dir }}/CogVideoX1.5-5B-I2V/model_index.json"
  register: cogvideox_i2v_final
  tags: [setup-cogvideox]

- name: Display I2V model download status
  debug:
    msg: |
      CogVideoX1.5-5B-I2V: {{ 'Downloaded ‚úÖ' if cogvideox_i2v_final.stat.exists else 'Failed ‚ùå' }}
      Location: {{ models_dir }}/CogVideoX1.5-5B-I2V/
  tags: [setup-cogvideox]

# ============================================
# STEP 6: CREATE GENERATION SCRIPT
# ============================================
- name: Create CogVideoX generation script
  copy:
    dest: "{{ models_dir }}/CogVideoX-generate.py"
    content: |
      #!/usr/bin/env python3
      """
      CogVideoX Video Generation Script
      Supports T2V (CogVideoX-5b) and I2V (CogVideoX1.5-5B-I2V)

      Usage:
        # Text-to-Video
        python CogVideoX-generate.py t2v "Your prompt here"
        python CogVideoX-generate.py t2v "Your prompt" --multi-gpu

        # Image-to-Video
        python CogVideoX-generate.py i2v "Your prompt" --image /path/to/image.png
        python CogVideoX-generate.py i2v "Your prompt" --image /path/to/image.png --multi-gpu
      """

      import argparse
      import os
      import time
      import torch
      from diffusers import CogVideoXPipeline, CogVideoXImageToVideoPipeline
      from diffusers.utils import export_to_video, load_image

      def get_gpu_count():
          """Get number of available GPUs"""
          return torch.cuda.device_count()

      def generate_t2v(prompt: str, output_dir: str = "/mnt/output",
                       multi_gpu: bool = False, quantize: bool = False,
                       output_name: str = None):
          """Generate video using CogVideoX-5b (Text-to-Video)"""

          timestamp = time.strftime("%Y%m%d_%H%M%S")
          output_name = output_name or f"cogvideox_t2v_{timestamp}"
          output_path = os.path.join(output_dir, f"{output_name}.mp4")

          print(f"üé¨ CogVideoX-5B Text-to-Video Generation")
          print(f"=" * 50)
          print(f"Prompt: {prompt}")
          print(f"Output: {output_path}")
          print(f"Multi-GPU: {multi_gpu}")
          print(f"Quantize: {quantize}")
          print(f"=" * 50)

          model_path = "/mnt/models/CogVideoX-5b"

          num_gpus = get_gpu_count()
          print(f"GPUs available: {num_gpus}")

          if quantize:
              print("Loading with INT8 quantization...")
              from transformers import T5EncoderModel
              try:
                  from torchao.quantization import quantize_, int8_weight_only

                  text_encoder = T5EncoderModel.from_pretrained(
                      model_path, subfolder="text_encoder", torch_dtype=torch.bfloat16
                  )
                  quantize_(text_encoder, int8_weight_only())

                  pipe = CogVideoXPipeline.from_pretrained(
                      model_path,
                      text_encoder=text_encoder,
                      torch_dtype=torch.bfloat16,
                  )
              except ImportError:
                  print("torchao not installed, using standard loading with CPU offload")
                  pipe = CogVideoXPipeline.from_pretrained(
                      model_path,
                      torch_dtype=torch.bfloat16,
                  )
              pipe.enable_sequential_cpu_offload()

          elif multi_gpu and num_gpus > 1:
              print(f"Loading model across {num_gpus} GPUs...")
              pipe = CogVideoXPipeline.from_pretrained(
                  model_path,
                  torch_dtype=torch.bfloat16,
                  device_map="balanced",
              )

          else:
              print("Loading model with CPU offload (single GPU)...")
              pipe = CogVideoXPipeline.from_pretrained(
                  model_path,
                  torch_dtype=torch.bfloat16,
              )
              pipe.enable_model_cpu_offload()

          pipe.vae.enable_tiling()
          pipe.vae.enable_slicing()

          print("Generating video...")
          start_time = time.time()

          video = pipe(
              prompt=prompt,
              num_videos_per_prompt=1,
              num_inference_steps=50,
              num_frames=49,
              guidance_scale=6,
              generator=torch.Generator(device="cuda").manual_seed(int(time.time()) % 1000000),
          ).frames[0]

          elapsed = time.time() - start_time
          print(f"Generation completed in {elapsed:.1f} seconds")

          export_to_video(video, output_path, fps=8)
          print(f"‚úÖ Video saved: {output_path}")

          return output_path

      def generate_i2v(prompt: str, image_path: str, output_dir: str = "/mnt/output",
                       multi_gpu: bool = False, quantize: bool = False,
                       output_name: str = None, num_frames: int = 81):
          """Generate video using CogVideoX1.5-5B-I2V (Image-to-Video)"""

          timestamp = time.strftime("%Y%m%d_%H%M%S")
          output_name = output_name or f"cogvideox_i2v_{timestamp}"
          output_path = os.path.join(output_dir, f"{output_name}.mp4")

          print(f"üé¨ CogVideoX1.5-5B Image-to-Video Generation")
          print(f"=" * 50)
          print(f"Prompt: {prompt}")
          print(f"Image: {image_path}")
          print(f"Output: {output_path}")
          print(f"Frames: {num_frames} (~{num_frames/8:.1f}s @ 8fps)")
          print(f"Multi-GPU: {multi_gpu}")
          print(f"Quantize: {quantize}")
          print(f"=" * 50)

          model_path = "/mnt/models/CogVideoX1.5-5B-I2V"

          # Load the input image
          image = load_image(image_path)
          print(f"Image loaded: {image.size}")

          num_gpus = get_gpu_count()
          print(f"GPUs available: {num_gpus}")

          # Note: device_map doesn't work with I2V - always use CPU offload
          print(f"Loading model with aggressive CPU offload...")
          if multi_gpu:
              print("Note: Multi-GPU device_map not compatible with I2V - using CPU offload")

          pipe = CogVideoXImageToVideoPipeline.from_pretrained(
              model_path,
              torch_dtype=torch.bfloat16,
          )
          pipe.enable_model_cpu_offload(gpu_id=0)

          pipe.vae.enable_tiling()
          pipe.vae.enable_slicing()

          print("Generating video...")
          start_time = time.time()

          video = pipe(
              prompt=prompt,
              image=image,
              num_videos_per_prompt=1,
              num_inference_steps=50,
              num_frames=num_frames,
              guidance_scale=6,
              generator=torch.Generator(device="cuda").manual_seed(int(time.time()) % 1000000),
          ).frames[0]

          elapsed = time.time() - start_time
          print(f"Generation completed in {elapsed:.1f} seconds")

          export_to_video(video, output_path, fps=8)
          print(f"‚úÖ Video saved: {output_path}")

          return output_path

      if __name__ == "__main__":
          parser = argparse.ArgumentParser(description="CogVideoX Video Generation (T2V + I2V)")
          parser.add_argument("mode", type=str, choices=["t2v", "i2v"], help="Generation mode: t2v or i2v")
          parser.add_argument("prompt", type=str, help="Text prompt for video generation")
          parser.add_argument("--image", type=str, default=None, help="Input image path (required for i2v)")
          parser.add_argument("--output-dir", type=str, default="/mnt/output", help="Output directory")
          parser.add_argument("--output-name", type=str, default=None, help="Output filename (without extension)")
          parser.add_argument("--multi-gpu", action="store_true", help="Use all available GPUs")
          parser.add_argument("--quantize", action="store_true", help="Use INT8 quantization for low VRAM")
          parser.add_argument("--frames", type=int, default=81, help="Number of frames for I2V (default: 81 = 10s)")

          args = parser.parse_args()

          if args.mode == "t2v":
              generate_t2v(
                  prompt=args.prompt,
                  output_dir=args.output_dir,
                  multi_gpu=args.multi_gpu,
                  quantize=args.quantize,
                  output_name=args.output_name,
              )
          elif args.mode == "i2v":
              if not args.image:
                  print("‚ùå Error: --image is required for i2v mode")
                  exit(1)
              if not os.path.exists(args.image):
                  print(f"‚ùå Error: Image not found: {args.image}")
                  exit(1)
              generate_i2v(
                  prompt=args.prompt,
                  image_path=args.image,
                  output_dir=args.output_dir,
                  multi_gpu=args.multi_gpu,
                  quantize=args.quantize,
                  output_name=args.output_name,
                  num_frames=args.frames,
              )
    owner: ubuntu
    group: ubuntu
    mode: '0755'
  tags: [setup-cogvideox]

# ============================================
# STEP 7: CREATE WRAPPER SCRIPT
# ============================================
- name: Create cogvideox command wrapper
  copy:
    dest: /usr/local/bin/cogvideox-generate
    content: |
      #!/bin/bash
      # CogVideoX Generation Wrapper (T2V + I2V)
      source /mnt/models/CogVideoX-venv/bin/activate
      python3 /mnt/models/CogVideoX-generate.py "$@"
    mode: '0755'
  tags: [setup-cogvideox]

# ============================================
# FINAL STATUS
# ============================================
- name: Verify complete installation
  shell: |
    source {{ models_dir }}/CogVideoX-venv/bin/activate
    echo "PyTorch: $(python -c 'import torch; print(torch.__version__)')"
    echo "CUDA available: $(python -c 'import torch; print(torch.cuda.is_available())')"
    echo "GPU count: $(python -c 'import torch; print(torch.cuda.device_count())')"
    echo "Diffusers: $(python -c 'import diffusers; print(diffusers.__version__)')"
  become_user: ubuntu
  args:
    executable: /bin/bash
  register: install_verify
  tags: [setup-cogvideox]
  ignore_errors: yes

- name: Display setup complete
  debug:
    msg: |
      ==========================================
      ‚úÖ COGVIDEOX SETUP COMPLETE (T2V + I2V)
      ==========================================

      üìÅ Locations:
         Venv:      {{ models_dir }}/CogVideoX-venv
         T2V Model: {{ models_dir }}/CogVideoX-5b
         I2V Model: {{ models_dir }}/CogVideoX1.5-5B-I2V
         Script:    {{ models_dir }}/CogVideoX-generate.py

      üîç Installation Check:
      {{ install_verify.stdout }}

      üé¨ Usage - Text-to-Video (T2V):
         cogvideox-generate t2v "Your prompt here"
         cogvideox-generate t2v "Your prompt" --multi-gpu
         cogvideox-generate t2v "Your prompt" --quantize

      üé¨ Usage - Image-to-Video (I2V):
         cogvideox-generate i2v "Your prompt" --image /path/to/image.png
         cogvideox-generate i2v "Your prompt" --image img.png --multi-gpu
         cogvideox-generate i2v "Your prompt" --image img.png --frames 49

      üì∫ Or via generate-video:
         generate-video cogvideox t2v "Your prompt"
         generate-video cogvideox i2v "Your prompt" --image img.png

      ‚ö° Performance:
         T2V: 49 frames @ 8fps = 6 seconds
         I2V: 81 frames @ 8fps = 10 seconds (default)
              49 frames @ 8fps = 6 seconds (--frames 49)

      ==========================================
  tags: [setup-cogvideox]
