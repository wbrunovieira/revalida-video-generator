# Wan 2.2: Guia de Uso

## üìã O que √©

Wan 2.2 √© um modelo da Wan-AI com arquitetura **MoE (Mixture-of-Experts)**, sendo o mais **vers√°til** dos modelos, suportando tanto **text-to-video** quanto **image-to-video**.

**Diferencial:** Flexibilidade - aceita texto, imagem, ou ambos como entrada.

## üéØ Caracter√≠sticas Principais

- **Dual-mode**: Text-to-Video (T2V) + Image-to-Video (I2V)
- **MoE Architecture**: Eficiente e poderosa
- **14B par√¢metros**: Modelo robusto
- **Qualidade profissional**: Boa para produ√ß√£o
- **At√© 10 segundos**: Por gera√ß√£o

## üì¶ Modelo j√° est√° instalado

O Ansible j√° baixou o modelo em `/mnt/models/Wan2.2/`

```
/mnt/models/Wan2.2/
‚îú‚îÄ‚îÄ models_t5_umt5-xxl-enc-bf16.pth  # Text encoder
‚îú‚îÄ‚îÄ Wan2.1_VAE.pth                    # VAE
‚îú‚îÄ‚îÄ high_noise_model/                 # DiT high noise
‚îî‚îÄ‚îÄ low_noise_model/                  # DiT low noise
```

## üöÄ Como Usar

### 1. Conectar ao servidor

```bash
make ssh
```

### 2. Ativar ambiente

```bash
venv
cd /mnt/output
```

### 3. Text-to-Video (T2V)

Crie um script Python `generate_wan_t2v.py`:

```python
import torch
from diffusers import WanVideoPipeline

# Carregar modelo T2V
pipe = WanVideoPipeline.from_pretrained(
    "/mnt/models/Wan2.2",
    torch_dtype=torch.float16,
    device_map="balanced"
)

# Gerar v√≠deo
prompt = "Um astronauta flutua no espa√ßo, terra ao fundo, estrelas cintilantes, cinematogr√°fico"

video = pipe(
    prompt=prompt,
    num_frames=240,  # ~10 segundos
    height=720,
    width=1280,
    num_inference_steps=50,
    guidance_scale=7.0,
).frames[0]

# Salvar
from diffusers.utils import export_to_video
export_to_video(video, "astronauta.mp4", fps=24)
```

### 4. Image-to-Video (I2V)

Crie um script Python `generate_wan_i2v.py`:

```python
import torch
from PIL import Image
from diffusers import WanVideoPipeline

# Carregar modelo I2V
pipe = WanVideoPipeline.from_pretrained(
    "/mnt/models/Wan2.2",
    torch_dtype=torch.float16,
    device_map="balanced"
)

# Carregar imagem inicial
image = Image.open("primeira_frame.jpg")

# Gerar v√≠deo a partir da imagem
prompt = "A imagem ganha vida, flores balan√ßam com o vento, suave e natural"

video = pipe(
    prompt=prompt,
    image=image,  # Primeira frame fixa
    num_frames=240,
    height=720,
    width=1280,
    num_inference_steps=50,
    guidance_scale=7.0,
).frames[0]

# Salvar
from diffusers.utils import export_to_video
export_to_video(video, "flores_animadas.mp4", fps=24)
```

Execute:
```bash
python generate_wan_t2v.py
# ou
python generate_wan_i2v.py
```

## ‚úçÔ∏è Dicas de Prompt

### Text-to-Video (T2V)

**Estrutura:**
```
[Sujeito] + [A√ß√£o] + [Ambiente] + [Movimento de c√¢mera] + [Estilo]
```

**Exemplos:**
```
# Natureza
Cachoeira fluindo em floresta tropical, p√°ssaros voando, c√¢mera lenta, dourado hour

# Urbano
Rua de Tokyo √† noite, chuva, luzes neon refletindo, c√¢mera em movimento, cyberpunk

# Abstrato
Tinta colorida se espalhando na √°gua, movimento fluido, macro, art√≠stico
```

### Image-to-Video (I2V)

**Foco no movimento:**
```
# Sutil
Folhas balan√ßam suavemente com a brisa, movimento natural, tranquilo

# Din√¢mico
Ondas gigantes quebrando na praia, dram√°tico, poderoso, c√¢mera fixa

# Atmosf√©rico
N√©voa se movendo entre as √°rvores, misterioso, cinematogr√°fico
```

**Dica:** No I2V, descreva o *movimento* que voc√™ quer, n√£o a cena completa (ela j√° est√° na imagem).

## ‚öôÔ∏è Par√¢metros Importantes

### N√∫mero de Frames

```python
num_frames=240    # ~10 segundos a 24 FPS (padr√£o)
num_frames=120    # ~5 segundos (mais r√°pido)
num_frames=480    # ~20 segundos (requer muita VRAM)
```

### Resolu√ß√£o

```python
# Qualidade alta
height=720, width=1280

# Balanceado
height=576, width=1024

# R√°pido
height=480, width=854
```

### Guidance Scale

```python
guidance_scale=7.0   # Padr√£o (recomendado)
guidance_scale=5.0   # Mais criativo
guidance_scale=9.0   # Mais fiel ao prompt
```

### Inference Steps

```python
num_inference_steps=50   # Bom equil√≠brio (padr√£o)
num_inference_steps=30   # Mais r√°pido
num_inference_steps=80   # Melhor qualidade
```

## üé® Workflows Avan√ßados

### 1. Pipeline Completo T2V + I2V

```python
# Passo 1: Gere primeira frame com Stable Diffusion
from diffusers import StableDiffusionPipeline

sd_pipe = StableDiffusionPipeline.from_pretrained("stabilityai/sd-xl-base-1.0")
first_frame = sd_pipe("Um castelo medieval ao p√¥r do sol").images[0]
first_frame.save("castelo.jpg")

# Passo 2: Anime a imagem com Wan I2V
wan_pipe = WanVideoPipeline.from_pretrained("/mnt/models/Wan2.2")
video = wan_pipe(
    prompt="Nuvens se movendo, luz mudando, p√°ssaros voando",
    image=first_frame,
    num_frames=240
).frames[0]
```

### 2. Sequ√™ncia de V√≠deos

```python
# Gere m√∫ltiplos clips e concatene
clips = []

for prompt in [
    "Amanhecer nas montanhas",
    "Sol nascendo lentamente",
    "P√°ssaros come√ßam a voar"
]:
    video = pipe(prompt=prompt, num_frames=120).frames[0]
    clips.append(video)

# Concatenar com FFmpeg ou edi√ß√£o
```

## üìä Requisitos de VRAM

### Text-to-Video (T2V)
- **720p, 240 frames:** ~60GB VRAM
- **576p, 240 frames:** ~40GB VRAM
- **480p, 240 frames:** ~30GB VRAM

### Image-to-Video (I2V)
- **720p, 240 frames:** ~50GB VRAM (menos que T2V)
- **576p, 240 frames:** ~35GB VRAM

## üìç Localiza√ß√£o dos V√≠deos

V√≠deos gerados ficam em `/mnt/output/`

Para copiar para sua m√°quina local:
```bash
scp -i ~/.ssh/id_rsa ubuntu@IP:/mnt/output/*.mp4 ~/Downloads/
```

## üîß Troubleshooting

### Erro de mem√≥ria

**T2V:**
```python
# Reduzir frames
num_frames=120  # ao inv√©s de 240

# Reduzir resolu√ß√£o
height=576, width=1024
```

**I2V:**
```python
# I2V usa menos mem√≥ria que T2V
# Geralmente √© mais est√°vel
```

### V√≠deo muito est√°tico (I2V)

- Seja mais espec√≠fico no prompt sobre o movimento
- Use palavras como: `din√¢mico`, `movimento`, `fluindo`
- Aumente `guidance_scale` para 8-9

### Primeira frame diferente da imagem (I2V)

- Reduza `guidance_scale` para 5-6
- Adicione `high fidelity` ao prompt
- Verifique se a imagem est√° no formato correto (RGB, n√£o RGBA)

## üìö Recursos

- **Hugging Face:** https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B
- **GitHub:** https://github.com/Wan-AI/Wan
- **Discord:** Comunidade ativa para suporte

## üí° Quando Usar Wan 2.2

**Use T2V quando:**
- Quer explorar conceitos novos
- N√£o tem imagem de refer√™ncia
- Quer varia√ß√£o criativa

**Use I2V quando:**
- Tem uma imagem perfeita j√° pronta
- Quer controle exato da primeira frame
- Quer animar arte/foto existente
- Precisa de consist√™ncia visual precisa

**Vantagens sobre outros modelos:**
- ‚úÖ Dois modos (T2V + I2V)
- ‚úÖ Eficiente (MoE architecture)
- ‚úÖ Vers√°til para diferentes workflows

## üìÑ Licen√ßa

Wan AI Open Source License

**Verifique termos de uso comercial no reposit√≥rio oficial.**
