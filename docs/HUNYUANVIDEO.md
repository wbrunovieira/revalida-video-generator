# HunyuanVideo: Guia de Uso

## üìã O que √©

HunyuanVideo √© um modelo da Tencent com **melhor qualidade visual** entre os modelos open-source, gerando v√≠deos em **1280√ó720 a 30 FPS**.

**Diferencial:** Suporta LoRA training para consist√™ncia de personagens entre v√≠deos diferentes.

## üéØ Caracter√≠sticas Principais

- **Melhor qualidade visual**: 720p nativo, 30 FPS
- **Suporte a LoRA**: Treine com imagens de refer√™ncia para personagens consistentes
- **13B par√¢metros**: Modelo grande e poderoso
- **3D Causal VAE**: Melhor compress√£o temporal
- **At√© 5 segundos**: Por gera√ß√£o individual

## üì¶ Modelo j√° est√° instalado

O Ansible j√° baixou o modelo em `/mnt/models/HunyuanVideo/`

```
/mnt/models/HunyuanVideo/
‚îú‚îÄ‚îÄ model_index.json
‚îú‚îÄ‚îÄ transformer/
‚îú‚îÄ‚îÄ vae/
‚îú‚îÄ‚îÄ text_encoder/
‚îî‚îÄ‚îÄ scheduler/
```

## üöÄ Como Usar

### 1. Conectar ao servidor

```bash
make ssh
```

### 2. Ativar ambiente

```bash
venv
cd /mnt/output  # V√≠deos ser√£o salvos aqui
```

### 3. Usar com Diffusers (Hugging Face)

Crie um script Python `generate_hunyuan.py`:

```python
import torch
from diffusers import HunyuanVideoPipeline

# Carregar modelo
pipe = HunyuanVideoPipeline.from_pretrained(
    "/mnt/models/HunyuanVideo",
    torch_dtype=torch.float16,
    device_map="balanced"  # Distribui entre as 4 GPUs
)

# Gerar v√≠deo
prompt = "Uma borboleta azul voa em um jardim florido, c√¢mera lenta, fotorealista, luz natural"

video = pipe(
    prompt=prompt,
    num_frames=129,  # ~5 segundos a 30 FPS
    height=720,
    width=1280,
    num_inference_steps=50,
    guidance_scale=7.5,
).frames[0]

# Salvar
from diffusers.utils import export_to_video
export_to_video(video, "output.mp4", fps=30)
```

Execute:
```bash
python generate_hunyuan.py
```

## ‚úçÔ∏è Dicas de Prompt

### Estrutura Recomendada

```
[Sujeito] + [A√ß√£o] + [Ambiente] + [Estilo] + [Qualidade]
```

### Exemplos

**Natureza:**
```
Um tigre caminha pela floresta tropical, neblina matinal, cinematogr√°fico, 4K, fotorealista
```

**Urbano:**
```
Carro esportivo vermelho atravessa rua molhada √† noite, neon lights, c√¢mera lenta, alta qualidade
```

**Fantasia:**
```
Drag√£o voando sobre montanhas ao p√¥r do sol, escamas brilhantes, √©pico, estilo cinema
```

### Palavras-chave que Funcionam Bem

- **Qualidade:** `fotorealista`, `4K`, `alta qualidade`, `cinematogr√°fico`
- **Movimento:** `c√¢mera lenta`, `movimento suave`, `din√¢mico`
- **Ilumina√ß√£o:** `luz natural`, `golden hour`, `neon lights`, `contraluz`
- **Estilo:** `estilo cinema`, `professional`, `detailed`

## ‚öôÔ∏è Par√¢metros Importantes

### N√∫mero de Frames

```python
num_frames=129    # ~5 segundos (recomendado)
num_frames=65     # ~2.5 segundos (mais r√°pido)
num_frames=257    # ~10 segundos (requer mais VRAM)
```

### Resolu√ß√£o

```python
# Qualidade m√°xima (padr√£o)
height=720, width=1280

# Menor VRAM
height=480, width=854
```

### Inference Steps

```python
num_inference_steps=50   # Boa qualidade (padr√£o)
num_inference_steps=30   # Mais r√°pido, qualidade ok
num_inference_steps=100  # Melhor qualidade, mais lento
```

### Guidance Scale

```python
guidance_scale=7.5   # Padr√£o equilibrado
guidance_scale=5.0   # Mais criativo
guidance_scale=10.0  # Mais fiel ao prompt
```

## üé® LoRA Training (Consist√™ncia de Personagens)

### Workflow Recomendado

1. **Gerar 30 imagens de refer√™ncia** do personagem
2. **Treinar LoRA** com essas imagens (usar ferramentas como Kohya SS)
3. **Aplicar LoRA** em todas as gera√ß√µes de v√≠deo

### Exemplo com LoRA

```python
from diffusers import HunyuanVideoPipeline

pipe = HunyuanVideoPipeline.from_pretrained(
    "/mnt/models/HunyuanVideo",
    torch_dtype=torch.float16,
    device_map="balanced"
)

# Carregar LoRA treinado
pipe.load_lora_weights("/mnt/models/lora/meu_personagem.safetensors")

# Gerar com personagem consistente
video = pipe(
    prompt="meu_personagem caminhando na praia ao p√¥r do sol",
    num_frames=129,
    height=720,
    width=1280,
).frames[0]
```

## üìä Requisitos de VRAM

- **720p, 129 frames:** ~80GB VRAM (ok com 4x A10G)
- **480p, 129 frames:** ~40GB VRAM
- **720p, 65 frames:** ~40GB VRAM

## üìç Localiza√ß√£o dos V√≠deos

V√≠deos gerados ficam em `/mnt/output/`

Para copiar para sua m√°quina local:
```bash
scp -i ~/.ssh/id_rsa ubuntu@IP:/mnt/output/*.mp4 ~/Downloads/
```

## üîß Troubleshooting

### Erro de mem√≥ria (CUDA out of memory)

**Solu√ß√£o 1:** Reduzir frames
```python
num_frames=65  # ao inv√©s de 129
```

**Solu√ß√£o 2:** Reduzir resolu√ß√£o
```python
height=480, width=854  # ao inv√©s de 720√ó1280
```

**Solu√ß√£o 3:** Usar model offloading
```python
pipe.enable_model_cpu_offload()
```

### V√≠deo com artefatos

- Aumentar `num_inference_steps` para 70-100
- Ajustar `guidance_scale` (testar 5.0-10.0)
- Melhorar o prompt (ser mais espec√≠fico)

### Movimento n√£o natural

- Adicionar `smooth motion` ao prompt
- Reduzir n√∫mero de frames (menos movimento por segundo)
- Usar `slow motion` ou `c√¢mera lenta` no prompt

## üìö Recursos

- **Hugging Face:** https://huggingface.co/tencent/HunyuanVideo
- **GitHub:** https://github.com/Tencent/HunyuanVideo
- **Paper:** [HunyuanVideo: A Systematic Framework For Large Video Generation Models]

## üí° Combina√ß√£o com Outros Modelos

### Upscale com Video Upscaler

```bash
# Gerar em 480p (mais r√°pido)
python generate_hunyuan.py --height 480 --width 854

# Fazer upscale para 1080p com outro modelo
python upscale_video.py input.mp4 output_1080p.mp4
```

### Multi-shot com Edi√ß√£o

1. Gere m√∫ltiplos v√≠deos com HunyuanVideo
2. Use LoRA para manter personagem consistente
3. Edite no After Effects ou Premiere

## üìÑ Licen√ßa

Tencent Open Source License

**Verifique termos de uso comercial no reposit√≥rio oficial.**
